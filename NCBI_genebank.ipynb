{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMKICWpi5INV"
      },
      "outputs": [],
      "source": [
        "!pip install biopython bs4 requests Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AW7vWaDSC5cY"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNGvr8fd0oFQ"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import ftplib\n",
        "import requests\n",
        "import subprocess\n",
        "from bs4 import BeautifulSoup\n",
        "from itertools import islice\n",
        "import pandas as pd\n",
        "from Bio import SeqIO\n",
        "from datasets import load_dataset\n",
        "import time\n",
        "import subprocess\n",
        "import glob\n",
        "from Bio import SeqIO\n",
        "import pandas as pd\n",
        "import concurrent.futures\n",
        "import concurrent.futures\n",
        "import glob\n",
        "import subprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrGaYWpA-7dU"
      },
      "outputs": [],
      "source": [
        "def batched(iterable, n):\n",
        "    # batched('ABCDEFG', 3) --> ABC DEF G\n",
        "    if n < 1:\n",
        "        raise ValueError('n must be at least one')\n",
        "    it = iter(iterable)\n",
        "    while batch := tuple(islice(it, n)):\n",
        "        yield batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1vuYY_J5Cus"
      },
      "outputs": [],
      "source": [
        "def calculate_total_size(url):\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()  # Ensure we got a valid response\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Regular expression to find numbers followed by an 'M'\n",
        "    pattern = re.compile(r'(\\d+(\\.\\d+)?)M')\n",
        "    matches = pattern.findall(str(soup))\n",
        "\n",
        "    # Extract the first item from each tuple to get the numbers\n",
        "    sizes = [match[0] for match in matches]\n",
        "    sizes = [float(size) for size in sizes]\n",
        "    print(sizes)\n",
        "    return f'total size of files on the site is: {sum(sizes)}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wcmbuoyc4tPD"
      },
      "outputs": [],
      "source": [
        "def get_links(url):\n",
        "    # Fetch the content of the page\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()  # Ensure we got a valid response\n",
        "\n",
        "    # Parse the page using BeautifulSoup\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Find all anchor tags (links) and filter those ending with .gz\n",
        "    gz_links = [link.get('href') for link in soup.find_all('a') if link.get('href', '').endswith('.gz')]\n",
        "    return gz_links\n",
        "\n",
        "url = \"https://ftp.ncbi.nlm.nih.gov/genbank/\"  # Replace this with the website you want to scrape\n",
        "gz_links = get_links(url)\n",
        "print(f\"The website {url} contains {len(gz_links)} .gz links.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xt-Xsi3j9SzP"
      },
      "outputs": [],
      "source": [
        "delete_parquets = f'rm /content/upload_folder/*.parquet'\n",
        "delete_sequences = 'rm *.seq'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KcyTJfls6jrx"
      },
      "outputs": [],
      "source": [
        "def gunzip_file(file):\n",
        "    \"\"\"Function to decompress a single file.\"\"\"\n",
        "    subprocess.run(f\"gunzip {file}\", shell=True)\n",
        "\n",
        "def process_file(file, file_count):\n",
        "    seqs_data = {\n",
        "        'id': [],\n",
        "        'sequence': [],\n",
        "        'name': [],\n",
        "        'description': [],\n",
        "        'features': [],\n",
        "        'seq_length': []\n",
        "    }\n",
        "    try:\n",
        "        for record in SeqIO.parse(file, \"genbank\"):\n",
        "            seqs_data['id'].append(str(record.id))\n",
        "            seqs_data['sequence'].append(str(record.seq))\n",
        "            seqs_data['name'].append(str(record.name))\n",
        "            seqs_data['description'].append(str(record.description))\n",
        "            seqs_data['features'].append(len(record.features))\n",
        "            seqs_data['seq_length'].append(len(str(record.seq)))\n",
        "    except Exception as e:\n",
        "        print(f'{file} has an error: {e}')\n",
        "\n",
        "    count = file_count[file]  # Assuming `file_count` is a dict that maps filenames to an index\n",
        "    df = pd.DataFrame(seqs_data)\n",
        "    df.to_parquet(f'/content/upload_folder/file_{count}.parquet')\n",
        "    return f\"function call finished for {file}\"\n",
        "\n",
        "def process_batch(batch):\n",
        "    for link in batch:\n",
        "        command = f'wget https://ftp.ncbi.nlm.nih.gov/genbank/{link}'\n",
        "        subprocess.run(command, shell=True)\n",
        "\n",
        "    subprocess.run(f\"{delete_parquets} && {delete_sequences}\", shell=True)\n",
        "    gz_files = glob.glob(\"**.gz\")\n",
        "    with concurrent.futures.ProcessPoolExecutor() as executor:\n",
        "        executor.map(gunzip_file, gz_files)\n",
        "\n",
        "    files = glob.glob(\"*.seq\")\n",
        "    # Create a mapping from file name to a unique index\n",
        "    file_count = {f: idx for idx, f in enumerate(files)}\n",
        "\n",
        "    with concurrent.futures.ProcessPoolExecutor() as executor:\n",
        "        results = list(executor.map(process_file, files, [file_count]*len(files)))\n",
        "\n",
        "\n",
        "    for result in results:\n",
        "        print(result)\n",
        "\n",
        "    print('loading parquet files')\n",
        "    dataset = load_dataset(\"parquet\", data_files=\"/content/upload_folder/*.parquet\")\n",
        "    dataset.push_to_hub(f'Hack90/ncbi_genbank_part_{batch_count}')\n",
        "    dataset.cleanup_cache_files()\n",
        "    subprocess.run('rm -rf ~/.cache/huggingface/datasets', shell=True)\n",
        "batch_count = 1\n",
        "for batch in batched(gz_links, 100):\n",
        "    if batch_count > 7:\n",
        "        process_batch(batch)\n",
        "    batch_count += 1\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}