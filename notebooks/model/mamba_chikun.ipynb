{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6188fcde",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install lightning torch  ninja einops triton transformers causal_conv1d>=1.1.0 wandb Datasets mamba-ssm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nC9sYJJeyPhU",
      "metadata": {
        "id": "nC9sYJJeyPhU"
      },
      "outputs": [],
      "source": [
        "!wandb login\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "from pytorch_lightning import Trainer\n",
        "\n",
        "wandb_logger = WandbLogger(log_model=\"all\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae7e1002",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Dict, List, Tuple\n",
        "from torch import Tensor\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "from torch.utils.data import DataLoader\n",
        "from pytorch_lightning.callbacks import RichProgressBar\n",
        "from pytorch_lightning.callbacks import DeviceStatsMonitor\n",
        "from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "679bff7b",
      "metadata": {},
      "outputs": [],
      "source": [
        "tiny_model = {\n",
        "    \"d_model\": 768,\n",
        "    \"n_layer\": 24,\n",
        "    \"vocab_size\": 5,\n",
        "    \"ssm_cfg\": {},\n",
        "    \"rms_norm\": true,\n",
        "    \"residual_in_fp32\": true,\n",
        "    \"fused_add_norm\": true,\n",
        "    \"pad_vocab_size_multiple\": 8\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wvzGnQsWqdh2",
      "metadata": {
        "id": "wvzGnQsWqdh2"
      },
      "outputs": [],
      "source": [
        "def tokenize(dataset: Dataset) -> Tuple[Tensor, Dict]:\n",
        "    dictionary = Dictionary()\n",
        "\n",
        "\n",
        "    for sequence in dataset['train']['sequence']:\n",
        "            words = list(sequence) + [\"<eos>\"]\n",
        "            for word in words:\n",
        "                dictionary.add_word(word)\n",
        "    idss: List[Tensor] = []\n",
        "    # Tokenize file content\n",
        "    for sequence in dataset['train']['sequence']:\n",
        "            words = list(sequence) + [\"<eos>\"]\n",
        "            ids: List[int] = []\n",
        "            for word in words:\n",
        "                ids.append(dictionary.word2idx[word])\n",
        "            idss.append(torch.tensor(ids).type(torch.int64))\n",
        "    return torch.cat(idss), dictionary\n",
        "\n",
        "class Dictionary:\n",
        "    def __init__(self) -> None:\n",
        "        self.word2idx: Dict[str, int] = {}\n",
        "        self.idx2word: List[str] = []\n",
        "\n",
        "    def add_word(self, word: str) -> int:\n",
        "        if word not in self.word2idx:\n",
        "            self.idx2word.append(word)\n",
        "            self.word2idx[word] = len(self.idx2word) - 1\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.idx2word)\n",
        "    \n",
        "class VirusDataset(Dataset):\n",
        "    \"\"\"Mini version of WikiText2.\"\"\"\n",
        "\n",
        "    def __init__(self, dataset_name:str,  block_size: int = 1000) -> None:\n",
        "        super().__init__()\n",
        "        self.dataset = load_dataset(dataset_name)\n",
        "        self.data, self.dictionary = tokenize(self.dataset)\n",
        "        self.block_size = block_size\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self) -> int:\n",
        "        return len(self.dictionary)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.data) // self.block_size - 1\n",
        "\n",
        "    def __getitem__(self, index: int) -> Tuple[Tensor, Tensor]:\n",
        "        start = index * self.block_size\n",
        "        end = start + self.block_size\n",
        "        inputs = self.data[start:end]\n",
        "        return inputs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tsF-T5ggnuTI",
      "metadata": {
        "id": "tsF-T5ggnuTI"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class LanguageModel(pl.LightningModule):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.model = MambaLMHeadModel(**tiny_model\n",
        "        )\n",
        "\n",
        "    def training_step(self, batch):\n",
        "        input = batch\n",
        "        lm_logits = self.model(input).logits\n",
        "        labels = input.to(lm_logits.device)\n",
        "        shift_logits = lm_logits[:, :-1, :].contiguous()\n",
        "        labels = labels[:, 1:].contiguous()\n",
        "\n",
        "        loss_fct = torch.nn.CrossEntropyLoss()\n",
        "        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), labels.view(-1))\n",
        "\n",
        "        self.log(\"train_loss\", loss, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=0.1)\n",
        "\n",
        "pl.seed_everything(42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qLzg01QqOGz8",
      "metadata": {
        "id": "qLzg01QqOGz8"
      },
      "outputs": [],
      "source": [
        "# Data\n",
        "dataset  = VirusDataset(dataset_name= 'Hack90/chikungunya')\n",
        "train_dataloader = DataLoader(dataset, batch_size=512, num_workers=7)\n",
        "\n",
        "# Model\n",
        "model = LanguageModel()\n",
        "\n",
        "# Trainer\n",
        "trainer = pl.Trainer(accelerator=\"cuda\", devices=1, logger=wandb_logger)\n",
        "trainer.fit(model, train_dataloader)\n",
        "trainer.print(f\"Memory used: {torch.cuda.max_memory_allocated() / 1e9:.02f} GB\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
